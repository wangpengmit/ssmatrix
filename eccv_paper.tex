% last updated in April 2002 by Antje Endemann
% Based on CVPR 07 and LNCS, with modifications by DAF, AZ and elle, 2008 and AA, 2010, and CC, 2011

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,mathtools} % define this before the line numbering.
\usepackage{ruler}
\usepackage{color}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}


%% Generic latex macros macx
\def\eqref#1{(\ref{eq:#1})}
\def\eqlabel#1{\label{eq:#1}}
%\let\be=\equation
%\let\ee=\endequation
\def\tr{^\top}
\def\xcomment#1{\textcolor[gray]{.2}{\text{\em---#1}}}
\def\comment#1{\kern-1cm\xcomment{#1}}

\def\vec{\operatorname{vec}}
\DeclareMathOperator{\trace}{trace}

\def\hadamard{\odot}
\def\m#1{\ensuremath{\mathtt{#1}}}
\def\mt#1{\ensuremath{\mathtt{\tilde{#1}}}}
\def\v#1{\ensuremath{\mathbf{#1}}}

% paper-specific macros
\def\mU{\m U}
\def\mV{\m V}
\def\mW{\m W}
\def\mM{\m M}
\def\err{\epsilon}
\def\verr{\boldsymbol \epsilon}

\def\twiddle#1{{\tilde{#1}}}
\def\tU{\twiddle\mU}
\def\tW{\twiddle\mW}
\def\tV{\twiddle\mV}
\def\tVstar{{\twiddle\mV}^*}

\begin{document}
% \renewcommand\thelinenumber{\color[rgb]{0.2,0.5,0.8}\normalfont\sffamily\scriptsize\arabic{linenumber}\color[rgb]{0,0,0}}
% \renewcommand\makeLineNumber {\hss\thelinenumber\ \hspace{6mm} \rlap{\hskip\textwidth\ \hspace{6.5mm}\thelinenumber}}
% \linenumbers
\pagestyle{headings}
\mainmatter
\def \ECCV12SubNumber{1348}  % Insert your submission number here

\title{Exact-Wiberg Algorithm for Matrix Factorization with Missing Data} % Replace with your title

\titlerunning{ECCV-14 submission ID \ECCV12SubNumber}

\authorrunning{ECCV-14 submission ID \ECCV12SubNumber}

\author{Anonymous ECCV submission}
\institute{Paper ID \ECCV12SubNumber}


\maketitle

\begin{abstract}
We present a new derivation of the Wiberg algorithm for matrix factorization with missing data, showing how to compute the exact Jacobian of the projected problem, while previous research has used an approximate Jacobian.   The exact Jacobian, while sometimes considerably different to the approximation, makes little difference to convergence rates on real-world benchmarks.   The new derivation does, however, allow the derivation of a ``further damped'' Wiberg algorithm which can improve convergence on problems with very little data, and thus represents a new practical contribution to the state of the art in matrix factorization.
\end{abstract}


\section{Introduction}
For many applications of machine learning and computer vision, a key computational step is in the low-rank approximation of a given $m\times n$ matrix $\m M$, where some of the values are unobserved or, more generally, are accompanied with a weight matrix \mW.    This involves the minimization of the error function
\def\fro#1{\|#1\|_F^2}
\begin{equation}
\err(\m U,\m V) = \fro{\mW\hadamard(\m M -\m U \m V\tr)} + \frac\mu2(\fro \mU + \fro \mV)
\eqlabel{E}
\end{equation}
over unknown matrices $\m U, \m V$ each with $r$ columns.   The operator~$\hadamard$ is Hadamard (or elementwise) product, and the norms are Frobenius.  For this paper, we will focus on the 2-norm problem as stated above, but we expect that our observations will shed light on the 1-norm and other variants also.  We further note that although the question of choice of hyperparameters~$r, \mu$ is of great interest, our focus here is on finding the global optimum of~\eqref{E}, assuming the hyperparameters have been set.  Furthermore, there are other settings of the problem, for example using the nuclear norm, but as shown by Cabral et al.~\cite{cabral2013unifying}, there are many scenarios in which \eqref{E} is equivalent to nuclear norm factorization.

It is known that optimization of \eqref{E} with a nontrivial~$\mW$ is an intrinsically hard problem~\cite{cabral2013unifying}.   It is also known that for some standard representative datasets, it is hard in practice.   Recently however, great strides forward in the practical solution of these problems have been made using variants of the Wiberg algorithm~\cite{Wiberg76}.   In particular the family of damped Wiberg algorithms of Okatani et al.~\cite{okatani2011efficient} dramatically improved performance on real-world problems.   These algorithms may be loosely characterized as Levenberg-Marquardt algorithms applied to the function
\def\erru{\varepsilon}
\def\verru{{\boldsymbol\varepsilon}}
\begin{align}
\erru(\mU) = \min_\mV \err(\m U, \m V)
\end{align}
with a certain approximation to the Jacobian appearing in $\partial\erru/\partial\mU$.  Because the Wiberg algorithm derives the Jacobian in a somewhat oblique way, terms in the true Jacobian are discarded, and it has not previously been known what effect these neglected terms may have.   Generalizations of the Wiberg algorithm~\cite{strelow2012general} inherit this approximation, so our results on the specific case of matrix factorization may impact those generalizations, but that is beyond our scope here.

The contributions of this paper are as follows:
\begin{enumerate}
\item We provide a simple new derivation of the Wiberg algorithm directly applying (extensions of) the matrix calculus rules of~\cite{minka00}.
\item This new derivation allows us to easily include the derivative terms neglected in previous Wiberg-based methods, so that we are using the true analytic derivatives for the Levenberg-Marquardt algorithm.   We show that inclusion of these terms makes little difference to the performance of the algorithm, so that earlier approximations were indeed justified.
\item We further develop the relationship between Wiberg and Schur-complement Gauss-Newton, showing how there is opportunity for further damping in the Damped Wiberg algorithm~\cite{okatani2011efficient}, which further improves covergence.   This yields a practical algorithm for factorization which converges almost always in practice on problems once considered difficult.
\end{enumerate}

%-------------------------------------------------------------------------


\def\sym{\operatorname{sym}}
\def\inv#1{{#1}^{\mathsf{-1}}}
\def\mA{\m A}

\def\Id#1{\m{I}_{#1}}
\def\kron#1#2{{#1}\otimes{#2}}
\def\pinv#1{{{#1}^\dagger}}
\definecolor{diffcol}{rgb}{0,0,.35}
%\def\diff[#1]{\textcolor{diffcol}{\partial[}#1\textcolor{diffcol}{]}}
\def\diff[#1]{\textcolor{diffcol}{\partial[#1]}}

\def\equations1{
\begin{align}
\vec(\mW \hadamard (\mM - \mU \mV\tr))
  & = \tW \vec(\mM - \mU \mV\tr) & \comment{ Define $\tW := \operatorname{diag}(\vec\mW)$ }
\\& = \tW \vec(\mM) - \tW \vec(\mU \mV\tr)
\\& = \tW \v m - \tW (\kron{\Id n}{\mU}) \vec(\mV\tr) & \xcomment{ Define $\v m := \vec\mM$ }
\eqlabel{resvec1}
\\& = \tW\v m - \tW \twiddle \mU \vec(\mV\tr) &
\comment{ Define $\twiddle\mU := \kron{\Id n}{\mU}$}
\eqlabel{resvec2}
\end{align}
}

\begin{coq_example}
\coq_input{eccv_paper.style.coq.txt}
\coq_input{eccv_paper.coq.txt}
\end{coq_example}

\section{Matrix calculus rules}
As a preliminary step, we recall some standard matrix calculus rules from~\cite{minka00}, and derive some extensions which will prove useful in the following.   The derivative operator passes through matrix inverse as follows
\begin{equation}
\diff[\inv \mA] = -\inv \mA \diff[\mA] \inv \mA
\end{equation}
where the colouring of arguments has no mathematical significance, but makes it easier to follow the ``drilling down'' of the derivative operator.  

Defining the Moore-Penrose pseudoinverse as
\begin{equation}
\pinv \mA = \inv{(\mA\tr \mA)}\mA\tr
\end{equation}
we obtain a rule for differentiating through pseudoinverse (see appendix for derivation):
\begin{align}
\diff[\pinv \mA] = -\pinv \mA \diff[\mA] \pinv \mA +
\inv{(\mA\tr\mA)}\diff[\mA]\tr(\Id{}-\mA\pinv\mA)
\eqlabel{diffpinv}
\end{align}
and we observe that the second term is zero for invertible \mA.  We will also use a rule for $\mA \pinv \mA$ as follows:
\begin{align}
\diff[\mA \pinv \mA] = \sym((\Id{} - \mA\pinv \mA) \diff[\mA] \pinv \mA)
\end{align}
where $\sym(\mA) := \mA\tr + \mA$. 

%\def\mupinv#1{{{#1}^{-\mu}}}
%Defining the $\mu$-pseudoinverse as
%\begin{equation}
%\mupinv \mA = \inv{(\mA\tr \mA + \mu\Id{})}\mA\tr
%\end{equation}
%we obtain a very similar rule for differentiating through it:
%\begin{align}
%\diff[\mupinv \mA] = -\pinv \mA \diff[\mA] \pinv \mA +
%\inv{(\mA\tr\mA+\mu\Id{})}\diff[\mA]\tr(\Id{}-\mA\pinv\mA)
%\eqlabel{diffpinv}
%\end{align}
%and
%\begin{align}
%\diff[\mA \mupinv \mA] = \sym((\Id{} - \mA\mupinv \mA) \diff[\mA] \pinv \mA)
%\end{align}

\section{Method}
Observing that 

\vecdot

\equations1

\end{document}
